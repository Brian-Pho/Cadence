---
layout: post
title: "My quest for artificial intelligence"
---

This post logs my ongoing quest to create artificial general intelligence (AGI). I will continually update this post as I find and explore more interesting ideas for creating AGI.

---

- [Thoughts as of Dec 25, 2018](#thoughts-as-of-dec-25-2018)
- [Thoughts as of March 23, 2019](#thoughts-as-of-march-23-2019)
- [Thoughts as of May 9, 2019](#thoughts-as-of-may-9-2019)
- [Thoughts as of January 16, 2020](#thoughts-as-of-january-16-2020)

---

#### Thoughts as of Dec 25, 2018

I'm keeping a log of my journey on becoming an AI researcher and (hopefully) creating AGI.

I started this journey thinking that I'd go through my timeline of "tick-tock"ing computer science and neuroscience textbooks and posting notes to CR4-DL. However, after going through the first two textbooks, I came across the "Mastery" book that my sister left behind. Mastery has convinced me that I want to become a master AI research such as the likes of Einstein and Turing but for AI. It was also around this time that I knew I wasn't retaining and learning as much as I wanted from the textbooks and I searched for ways of learning to learn. I came across "Make It Stick" and it sold me on its conclusion of learning by distributed retrieval. After I read some excellent science fiction books (The three body problem series), I've returned back to my destiny, this time picking up "Peak".

With "Peak" completed, I think I'm done with this learning to learn and mastery/expertise path and I'm ready to tackle the actual AI research. There are a few other side skills that I want to learn (cooking, writing) but the meta-mastery is done. I'll always be reminded of Wade from "Death's End" in that we must keep advancing, stopping at nothing. The end goal is general artificial intelligence and nothing less.

The current plan is to devise a path to AI. I've brainstormed a few. One is to follow the evolution path and hope that creates AI but it has a low chance of success with a high time investment. Another is to follow the human path of starting at a child and working the ways up. It has a higher chance of success but is more difficult than the evolutionary path.

---

#### Thoughts as of March 23, 2019

I've edited my last journal entry for clarity. Man was it bad. Anyways, I just completed the cognitive science textbook a week ago and I'm working on editing the notes to be more presentable. A few health problems have come up and if I die, I hope someone will take up this mantle of building AI with the neuroscience and cog sci approach. This is because I've realized something about problems in general; that they have weak points. And I believe that the weak point of AI is our brains because it's the only known object to exhibit intelligent behavior.

I'm still working out the details of how to create AI but one item I've made progress on is that we'll need some measure to see how close we are to AGI. "If you can't measure it, you can't improve it." Whether that be the complexity of the game it can play, how many jobs it can replace, or how close it is to passing the Turing test, we need some measure. I don't know what measure is relevant or best, but since we're aiming for AI, a measure of intelligence is needed (IQ isn't a very good measure).

---

#### Thoughts as of May 9, 2019

A lot of interesting things have happened since my last entry. I'll start with the biggest one.

I recently came across the idea of "neuromorphic computing" which is computation that based on the human brain. I first encountered it in the book "Artificial Intelligence: Perspectives from Leading Practitioners" where Dharmendra Modha talks about it. The main principles of a neuromorphic chip are that:

- non-Von Neumann architecture aka there's no separation between processing and memory.
- Event-based not clock-based like our current computers.
- Extremely power efficient as it doesn't waste energy updating clock cycles.
- Massively parallel.

I believe this is the missing piece in the quest for AI. I've always had some reservations and skepticism about using digital computers as the platform for AI and neuromorphic chips are the answer. This is the hardware piece of AI but we also need the software piece.

I'm currently taking a psychology course called "cognitive psychology" and I'm really enjoying it. As I've learnt more about the brain, the more I believe that AI will have to be a combination of specialized components like the brain. I haven't gotten to the book on this, but the brain seems to be a "kludge" where different parts are specialized for different functions. For example, we have a specific region for face recognition and if it's destroyed, people lose the ability to recognize faces (medically called prosopagnosia).

Another idea that's been marinating in my mind is that we need a theory of cognition/mind. The theory would explain how behavior arises from neural activity. An example would be that the release of the prolactin neurotransmitter explains why men can't continually orgasm. See [this](https://www.reddit.com/r/explainlikeimfive/comments/bl0if3/eli5_why_does_a_mans_penis_hurt_if_he_continues/) for more details.

Speaking of how neural activity translates to behavior, I think the mind-body problem in philosophy is ridiculous and a waste of time. The argument is based on the assumption that the mind and body is separate. However, this isn't empirically true as things that I do to my body affect my mind and vice versa. For example, I can take drugs and that has an effect on my mind. I can also use my mind to control my body. This suggests that there's a link between my mind and body hence they're not separate.

---

#### Thoughts as of January 16, 2020

I haven't updated this journal in a while due to school and laziness but that doesn't mean nothing interesting has come up. Here are a few new developments.

The first is that I now believe that consciousness and intelligence are interlinked and that AI cannot be achieved without first building consciousness. My reasoning comes from two pieces of evidence. The first is from the book "The Feeling of What Happens" which details an evidence-based theory of consciousness. To summarize, the book hypotheses that consciousness is due to the brain applying feeling to the act of feeling. This means that the brain does a "map of a map" representation of itself and that is how we achieve consciousness. Furthermore, the book hypothesizes that consciousness is located in the brain stem because of various clinical cases and conditions. Further details can be found [here](https://brianpho.com/CR4-DL/books/2019/09/01/the-feeling-of-what-happens.html) but the main point is that with the absence of consciousness, no intelligent behavior occurs.

This leads me into my second piece of evidence in the form of a question. Have we ever seen an intelligent person without consciousness or vice versa? I don't think we have because the two are intimately linked. You can try it out yourself by trying to reason or plan without the use of your internal voice or internal eye. I'm not able to do anything because there is no guidance, no director. While the two may be separate given their definitions, it's hard to say that intelligence can exist without consciousness because we have not found examples of that.

The second development is a reoccurring theme that I've been noticing across various brain theories. The brain theories that I've recently looked into are Chris Eliasmith's Neural Engineering Framework, Jeff Hawkins' Thousand Brains Theory of Intelligence, and the After Digital textbook. The reoccurring themes are:

- The notion of representations
- The notion of information
- Using dimensions as features of the data
- The sparsity of representations
- The account of time

While I don't agree with any of the three theories/frameworks, they all seem to be getting at some common features of the brain that I think will be useful in building AI. It'll take more time and work to tease out the commonalities but the reoccurrence of such themes is probably hinting at something important.

Anyways, the third development is that I've been recently applying to masters programs in Canada, Specifically, I've applied to the following programs:

- Masters in Systems Design Engineering at University of Waterloo
- Masters in Neuroscience at McGill University
- Masters in Computer Science at McGill University
- Masters in Neuroscience at Western University

I've been applying because I need more time to self-study the brain and because I want to become a professional scientist. While this isn't directly AI related, this is the education that I need to undertake to push myself to learn more and to grow up and be independent.

---
